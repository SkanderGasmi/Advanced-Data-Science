{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "Welcome to Assignment 2. This will be fun. It is the first time you actually access external data from ApacheSpark.\n",
    "\n",
    "#### You can also submit partial solutions\n",
    "\n",
    "Just make sure you hit the play button on each cell from top to down. There are three functions you have to implement. Please also make sure than on each change on a function you hit the play button again on the corresponding cell to make it available to the rest of this notebook.\n",
    "\n",
    "> Note: Replace **##** placeholder with the relevent parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rklib in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (0.1.0)\n",
      "Requirement already satisfied: requests in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from rklib) (2.29.0)\n",
      "Requirement already satisfied: green in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from rklib) (3.4.3)\n",
      "Requirement already satisfied: pyyaml in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from rklib) (6.0)\n",
      "Requirement already satisfied: colorama in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from green->rklib) (0.4.6)\n",
      "Requirement already satisfied: coverage in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from green->rklib) (7.2.7)\n",
      "Requirement already satisfied: unidecode in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from green->rklib) (1.3.7)\n",
      "Requirement already satisfied: lxml in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from green->rklib) (4.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->rklib) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->rklib) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->rklib) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->rklib) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install rklib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first function you have to implement. You are passed a dataframe object. We've also registered the dataframe in the ApacheSparkSQL catalog - so you can also issue queries against the \"washing\" table using \"spark.sql()\". Hint: To get an idea about the contents of the catalog you can use: spark.catalog.listTables().\n",
    "So now it's time to implement your first function. You are free to use the dataframe API, SQL or RDD API. In case you want to use the RDD API just obtain the encapsulated RDD using \"df.rdd\". You can test the function by running one of the three last cells of this notebook, but please make sure you run the cells from top to down since some are dependant of each other...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Please implement a function returning the number of rows in the dataframe\n",
    "def count(spark, dataframe_name):\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    #some reference: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Working-with-SQL\n",
    "    #some more help: https://www.w3schools.com/sql/sql_count_avg_sum.asp    \n",
    "    \n",
    "    # Register the DataFrame as a temporary table\n",
    "    spark.sql(f\"CREATE OR REPLACE TEMPORARY VIEW {dataframe_name} AS SELECT * FROM {dataframe_name}\")\n",
    "\n",
    "    # Use Spark SQL to count the number of rows\n",
    "    result = spark.sql(f\"SELECT COUNT(*) as cnt FROM {dataframe_name}\")\n",
    "\n",
    "    # Return the count as an integer\n",
    "    return result.first().cnt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to implement the second function. Please return an integer containing the number of fields (columns). The most easy way to get this is using the dataframe API. Hint: You might find the dataframe API documentation useful: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def getNumberOfFields(## , ##):\n",
    "#     #TODO Please enter your code here, you are not required to use the template code below\n",
    "#     #some reference: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Working-with-SQL\n",
    "#     return len(df.##)\n",
    "               \n",
    "def getNumberOfFields(spark, dataframe_name):\n",
    "    # Register the DataFrame as a temporary table\n",
    "    spark.sql(f\"CREATE OR REPLACE TEMPORARY VIEW {dataframe_name} AS SELECT * FROM {dataframe_name}\")\n",
    "\n",
    "    # Use Spark SQL to get the schema of the DataFrame\n",
    "    result = spark.sql(f\"DESCRIBE {dataframe_name}\")\n",
    "\n",
    "    # Count the number of fields (columns)\n",
    "    num_fields = result.count()\n",
    "\n",
    "    return num_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, please implement a function which returns a (python) list of string values of the field names in this data frame. Hint: Just copy&past doesn't work because the auto-grader will create a random data frame for testing, so please use the data frame API as well. Again, this is the link to the documentation: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Working-with-SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def getFieldNames(## , ##):\n",
    "#     #TODO Please enter your code here, you are not required to use the template code below\n",
    "#     #some reference: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Working-with-SQL\n",
    "#     return df.##\n",
    "    \n",
    "def getFieldNames(spark, dataframe_name):\n",
    "    # Register the DataFrame as a temporary table\n",
    "    spark.sql(f\"CREATE OR REPLACE TEMPORARY VIEW {dataframe_name} AS SELECT * FROM {dataframe_name}\")\n",
    "\n",
    "    # Use Spark SQL to get the schema of the DataFrame\n",
    "    result = spark.sql(f\"DESCRIBE {dataframe_name}\")\n",
    "\n",
    "    # Extract the field names from the result DataFrame\n",
    "    field_names = [row.col_name for row in result.collect()]\n",
    "\n",
    "    return field_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to grab a PARQUET file and create a dataframe out of it. Using SparkSQL you can handle it like a database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-17 23:33:30--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-SN0101EN-SkillsNetwork/labs/Week2_FSDS/washing.parquet\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 112048 (109K) [binary/octet-stream]\n",
      "Saving to: ‘washing.parquet.2’\n",
      "\n",
      "washing.parquet.2   100%[===================>] 109.42K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2023-11-17 23:33:30 (42.4 MB/s) - ‘washing.parquet.2’ saved [112048/112048]\n",
      "\n",
      "mv: cannot stat 'washing.parquet?raw=true': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-SN0101EN-SkillsNetwork/labs/Week2_FSDS/washing.parquet\n",
    "!mv washing.parquet?raw=true washing.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n",
      "|                 _id|                _rev|count|flowrate|fluidlevel|frequency|hardness|speed|temperature|           ts|voltage|\n",
      "+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n",
      "|0d86485d0f88d1f9d...|1-57940679fb8a713...|    4|      11|acceptable|     null|      77| null|        100|1547808723923|   null|\n",
      "|0d86485d0f88d1f9d...|1-15ff3a0b304d789...|    2|    null|      null|     null|    null| 1046|       null|1547808729917|   null|\n",
      "|0d86485d0f88d1f9d...|1-97c2742b68c7b07...|    4|    null|      null|       71|    null| null|       null|1547808731918|    236|\n",
      "|0d86485d0f88d1f9d...|1-eefb903dbe45746...|   19|      11|acceptable|     null|      75| null|         86|1547808738999|   null|\n",
      "|0d86485d0f88d1f9d...|1-5f68b4c72813c25...|    7|    null|      null|       75|    null| null|       null|1547808740927|    235|\n",
      "|0d86485d0f88d1f9d...|1-cd4b6c57ddbe77e...|    5|    null|      null|     null|    null| 1014|       null|1547808744923|   null|\n",
      "|0d86485d0f88d1f9d...|1-a35b25b5bf43aaf...|   32|      11|acceptable|     null|      73| null|         84|1547808752028|   null|\n",
      "|0d86485d0f88d1f9d...|1-b717f7289a8476d...|   48|      11|acceptable|     null|      79| null|         84|1547808768065|   null|\n",
      "|0d86485d0f88d1f9d...|1-c2f1f8fcf178b2f...|   18|    null|      null|       73|    null| null|       null|1547808773944|    228|\n",
      "|0d86485d0f88d1f9d...|1-15033dd9eebb4a8...|   59|      11|acceptable|     null|      72| null|         96|1547808779093|   null|\n",
      "|0d86485d0f88d1f9d...|1-753dae825f9a6c2...|   62|      11|acceptable|     null|      73| null|         88|1547808782113|   null|\n",
      "|0d86485d0f88d1f9d...|1-b168089f44f03f0...|   13|    null|      null|     null|    null| 1097|       null|1547808784940|   null|\n",
      "|0d86485d0f88d1f9d...|1-403b687c6be0dea...|   23|    null|      null|       80|    null| null|       null|1547808788955|    236|\n",
      "|0d86485d0f88d1f9d...|1-195551e0455a24b...|   72|      11|acceptable|     null|      77| null|         87|1547808792134|   null|\n",
      "|0d86485d0f88d1f9d...|1-060a39fc6c2ddee...|   26|    null|      null|       62|    null| null|       null|1547808797959|    233|\n",
      "|0d86485d0f88d1f9d...|1-2234514bffee465...|   27|    null|      null|       61|    null| null|       null|1547808800960|    226|\n",
      "|0d86485d0f88d1f9d...|1-4265898bb401db0...|   82|      11|acceptable|     null|      79| null|         96|1547808802154|   null|\n",
      "|0d86485d0f88d1f9d...|1-2fbf7ca9a0425a0...|   94|      11|acceptable|     null|      73| null|         90|1547808814186|   null|\n",
      "|0d86485d0f88d1f9d...|1-203c0ee6d7fbd21...|   97|      11|acceptable|     null|      77| null|         88|1547808817190|   null|\n",
      "|0d86485d0f88d1f9d...|1-47e1965db94fcab...|  104|      11|acceptable|     null|      75| null|         80|1547808824198|   null|\n",
      "+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('washing.parquet')\n",
    "df.createOrReplaceTempView('washing')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be used to test your count function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1905723318.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_70/1905723318.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print(cnt)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "cnt = None\n",
    "nof = None\n",
    "fn = None\n",
    "\n",
    "cnt = count(## , ##)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be used to test your getNumberOfFields function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nof = getNumberOfFields(## , ##)\n",
    "print(nof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be used to test your getFieldNames function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn = getFieldNames(## , ##)\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you are done. So please submit your solutions to the grader now.\n",
    "\n",
    "# Start of Assignment-Submission\n",
    "\n",
    "The first thing we need to do is to install a little helper library for submitting the solutions to the coursera grader:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f rklib.py\n",
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-SN0101EN-SkillsNetwork/rklib.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to submit first solution. Please make sure that the token variable contains a valid submission token. You can obtain it from the coursera web page of the course using the grader section of this assignment.\n",
    "\n",
    "Please specify you email address you are using with cousera as well.\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-SN0101EN-SkillsNetwork/images/ML%20FS%20Grader%20screenshot1.png\" width=\"500\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'submit' from 'rklib' (/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/rklib/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/2270759521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrklib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmitAll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ho91rZfhdIXW4IU8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'submit' from 'rklib' (/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/rklib/__init__.py)"
     ]
    }
   ],
   "source": [
    "from rklib import submit, submitAll\n",
    "import json\n",
    "    \n",
    "key = \"ho91rZfhdIXW4IU8\"\n",
    "\n",
    "if type(23) != type(cnt):\n",
    "    raise ValueError('Please make sure that \"cnt\" is a number')\n",
    "\n",
    "if type(23) != type(nof):\n",
    "    raise ValueError('Please make sure that \"nof\" is a number')\n",
    "\n",
    "if type([]) != type(fn):\n",
    "    raise ValueError('Please make sure that \"fn\" is a list')\n",
    "\n",
    "email = \"mohamedskander.gasmi@esprit.tn\"\n",
    "token = \"VdbV3Fd8ChPV5nDE\" \n",
    "parts_data = {}\n",
    "parts_data[\"2FjQw\"] = json.dumps(cnt)\n",
    "parts_data[\"j8gMs\"] = json.dumps(nof)\n",
    "parts_data[\"xaauC\"] = json.dumps(fn)\n",
    "\n",
    "\n",
    "submitAll(email, token, key, parts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Congratulations! You've successfully finished this lab. <h3/>\n",
    "\n",
    "## Author(s)\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/romeo-kienzler-089b4557/\" target=\"_blank\">Romeo Kienzler\n",
    "</a>\n",
    "\n",
    "## Other Contributor(s)\n",
    "- <a href=\"https://www.linkedin.com/in/sowmyaa-gurusamy-b4a743202/\" target=\"_blank\">Sowmyaa Gurusamy</a>\n",
    "\n",
    "- [Malika Singla](\n",
    "https://www.linkedin.com/in/malika-goyal-04798622)\n",
    "- [Lakshmi Holla](https://www.linkedin.com/in/lakshmi-holla-b39062149)\n",
    "\n",
    "## Changelog\n",
    "\n",
    "| Date | Version | Changed by | Change Description |\n",
    "|------|--------|--------|---------|\n",
    "| 2023-09-15 | 1.0 | Sowmyaa Gurusamy | Created initial version |\n",
    "|\n",
    "\n",
    "<center><h3> © IBM Corporation 2023. All rights reserved. <h3/></center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
